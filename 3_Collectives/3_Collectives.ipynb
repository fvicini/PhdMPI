{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c49911",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "############# Markdown note ##################\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Use blue boxes for Tips and notes. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Use green boxes sparingly, and only for some specific purpose that the other boxes can't cover. For example, if you have a lot of related content to link to, maybe you decide to use green boxes for related links from each section of a notebook. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> Use yellow boxes for examples that are not inside code cells, or use for mathematical formulas if needed. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> In general, just avoid the red boxes. </div>\n",
    "\n",
    "<img src=\"<path>\" width=20% style=\"margin-left:auto; margin-right:auto\">\n",
    "<img src=\"<path>\" width=40% style=\"float: right;\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91509d85",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# reset all programs\n",
    "rm -rf debug*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371497d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MPI Collectives\n",
    "\n",
    "Collective Communications with **Message Passing Interface** (MPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e705ae09",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Communications involving groups of processes are called **collectives**.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><code>MPI 1.0-2.0</code> collective calls are blocking. <code>MPI-3</code> introduced <b>non-blocking</b> collectives.</div>\n",
    "\n",
    "They have the following characteristics:\n",
    "* **Every** process in the communicator shall call the collective function;\n",
    "* **No tags** are required.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Designed to replace loops of point-to-point calls to be <b>more efficient</b>. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35471a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Barriers\n",
    "\n",
    "To stops a group of processes until they are **synchronized**.\n",
    "\n",
    "* `MPI_Barrier`: see https://www.open-mpi.org/doc/v4.1/man3/MPI_Barrier.3.php\n",
    "\n",
    "<img src=\"./Images/barrier.png\" width=50% style=\"margin-left:auto; margin-right:auto\">\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>Severe</b> performance impact if used too often. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67973ed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Broadcast\n",
    "\n",
    "Broadcasts a message from a process to **all other processes** of the group.\n",
    "\n",
    "* `MPI_Bcast`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Bcast.3.php\n",
    "\n",
    "<img src=\"./Images/bcast.png\" width=40% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7dcd26",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile main_bcast.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    \n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    \n",
    "    MPI_Status status;\n",
    "    double a[2] = { 0.0, 0.0 };\n",
    "    if ( rank == 0 ) \n",
    "    {\n",
    "        a[0] = 2.1; \n",
    "        a[1] = 4.3;\n",
    "    }\n",
    "    \n",
    "    // send the information to all the other processes\n",
    "    MPI_Bcast(a, 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
    "    \n",
    "    std::cout<< \"Process \"<< rank<< \" \";\n",
    "    std::cout<< \"a \"<< a[0]<< \", \"<< a[1]<< std::endl; \n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf53e29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_bcast\n",
    "cd debug_bcast\n",
    "cmake -DSOURCES=\"main_bcast.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2f0b1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_bcast\n",
    "mpirun -np 4 3_Collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9e63e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gather and Scatter\n",
    "\n",
    "One process collects data elements from all the processes and stores them in rank order (**Gather**) and viceversa (**Scatter**).\n",
    "\n",
    "* `MPI_Gather`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Gather.3.php\n",
    "* `MPI_Scatter`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Scatter.3.php\n",
    "\n",
    "<img src=\"./Images/gather.png\" width=48% style=\"float: left;\">  \n",
    "<img src=\"./Images/scatter.png\" width=48% style=\"float: right;\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85f865",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile main_gather_scatter.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    \n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    \n",
    "    MPI_Status status;\n",
    "    int a[8] = { 0 };\n",
    "    if (rank == 0) \n",
    "    {\n",
    "        for (unsigned int i = 0; i < 8; i++)\n",
    "            a[i] = i + 1; \n",
    "    }\n",
    "    \n",
    "    // send the information to all the other processes\n",
    "    MPI_Scatter(a, 2, MPI_INT, a, 2, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "    \n",
    "    std::cout<< \"Before Process \"<< rank<< \" \";\n",
    "    std::cout<< \"a: \";\n",
    "    for (unsigned int i = 0; i < 8; i++)\n",
    "        std::cout<< (i == 0 ? \"\" : \", \")<< a[i];\n",
    "    std::cout<< std::endl;\n",
    "        \n",
    "    a[0] *= 2;\n",
    "    a[1] *= 2;\n",
    "    \n",
    "    // get the information from the other processes\n",
    "    MPI_Gather(a, 2, MPI_INT, a, 2, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "    \n",
    "    // stop processes to obtain a good output\n",
    "    MPI_Barrier(MPI_COMM_WORLD);\n",
    "    \n",
    "    std::cout<< \"After Process \"<< rank<< \" \";\n",
    "    std::cout<< \"a: \";\n",
    "    for (unsigned int i = 0; i < 8; i++)\n",
    "        std::cout<< (i == 0 ? \"\" : \", \")<< a[i];\n",
    "    std::cout<< std::endl;\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a580c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_gather_scatter\n",
    "cd debug_gather_scatter\n",
    "cmake -DSOURCES=\"main_gather_scatter.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a937128",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_gather_scatter\n",
    "mpirun -np 4 3_Collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca34d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GatherV and ScatterV\n",
    "\n",
    "More **complex** gather and scatter call where itâ€™s possible to define a different length of arrays.\n",
    "\n",
    "* `MPI_GatherV`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Gatherv.3.php\n",
    "* `MPI_ScatterV`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Scatterv.3.php\n",
    "\n",
    "<img src=\"./Images/scatterv.png\" width=50% style=\"margin-left:auto; margin-right:auto\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d670a36a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other collectives\n",
    "\n",
    "* `MPI_Allgather`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Allgather.3.php\n",
    "* `MPI_Alltoall`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Alltoall.3.php\n",
    "* `MPI_Allgatherv`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Allgatherv.3.php\n",
    "* `MPI_Alltoallv`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Alltoallv.3.php\n",
    "* ...\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"><b>NOTE</b>: expensive calls - use only when needed.</div>\n",
    "\n",
    "<img src=\"./Images/allgather.png\" width=48% style=\"float: left;\">  \n",
    "<img src=\"./Images/alltoall.png\" width=48% style=\"float: right;\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbd272",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reductions\n",
    "\n",
    "<img src=\"./Images/reduce.png\" width=23% style=\"float: right;\">  \n",
    "\n",
    "A **reduction** takes values from a group of processes and generates a **single value** with some operation (e.g. a `sum`, `average`, etc.).\n",
    "\n",
    "* collect data from different processes;\n",
    "* store the result on a single process or distribute the value to all processes\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Designed to <b>avoid race-conditions</b>.</div>\n",
    "\n",
    "* `MPI_Reduce`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Reduce.3.php\n",
    "* `MPI_Allreduce`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Allreduce.3.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8347dd5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reduction Operations\n",
    "<img src=\"./Images/reduction.png\" width=50% style=\"margin-left:auto; margin-right:auto\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a91bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile main_reduce.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    \n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    \n",
    "    MPI_Status status;\n",
    "    int a[2] = { rank, rank + 1 };\n",
    "    int sum[2] = { 0, 0 };\n",
    "    \n",
    "    std::cout<< \"Before Process \"<< rank<< \" \";\n",
    "    std::cout<< \"a: \"<< a[0]<< \", \"<< a[1]<< std::endl;\n",
    "    \n",
    "    MPI_Reduce(&a, &sum, 2, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
    "            \n",
    "    std::cout<< \"After Process \"<< rank<< \" \";\n",
    "    std::cout<< \"sum: \"<< sum[0]<< \", \"<< sum[1]<< std::endl;\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1589a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_reduce\n",
    "cd debug_reduce\n",
    "cmake -DSOURCES=\"main_reduce.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9250267",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_reduce\n",
    "mpirun -np 4 3_Collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe9bad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Collectives and Performance\n",
    "\n",
    "MPI vendors work **hard** to optimise collectives for parallel hardware.\n",
    "\n",
    "_Latency measurements_ (minimum time needed to transfer data) in different machines: image taken from: https://doi.org/10.1002/cpe.6769\n",
    "<img src=\"./Images/performance.png\" width=80% style=\"margin-left:auto; margin-right:auto\"> \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> <b>NOTE</b>: parallel scaling is often dictated by MPI collectives. </div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
