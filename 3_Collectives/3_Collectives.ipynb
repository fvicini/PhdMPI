{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c49911",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "############# Markdown note ##################\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Use blue boxes for Tips and notes. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Use green boxes sparingly, and only for some specific purpose that the other boxes can't cover. For example, if you have a lot of related content to link to, maybe you decide to use green boxes for related links from each section of a notebook. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> Use yellow boxes for examples that are not inside code cells, or use for mathematical formulas if needed. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> In general, just avoid the red boxes. </div>\n",
    "\n",
    "<img src=\"<path>\" width=20% style=\"margin-left:auto; margin-right:auto\">\n",
    "<img src=\"<path>\" width=40% style=\"float: right;\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91509d85",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# reset all programs\n",
    "rm -rf debug*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371497d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MPI Collectives\n",
    "\n",
    "Collective Communications with **Message Passing Interface** (MPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e705ae09",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Communications involving a **group** or **groups** of processes are called **collectives**.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><code>MPI 1.0-2.0</code> collective calls are blocking. <code>MPI-3</code> introduced <b>non-blocking</b> collectives. <code>MPI-4</code> introduced <b>persistent</b> collectives.</div>\n",
    "\n",
    "They have the following characteristics:\n",
    "* **Every** process in the communicator shall call the collective function;\n",
    "* **No tags** are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ac07d-7bb5-48d6-b728-3a5cc1d74445",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## More efficient...\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Designed to replace loops of point-to-point calls to be <b>more efficient</b>. </div>\n",
    "\n",
    "Is this true according to standard? ....\n",
    "\n",
    "> ...While vendors may write optimized collective routines\n",
    "matched to their architectures, a complete library of the collective communication\n",
    "routines can be written entirely using the MPI point-to-point communication func-\n",
    "tions and a few auxiliary functions..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e665546-9e03-47d8-a3a9-da275778d5f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intra or Inter Communicator\n",
    "\n",
    "Two types of **communicators**:\n",
    "\n",
    "* **intra-communicator**: identifier for a single group of MPI processes linked with a context\n",
    "* **inter-communicator**: identifies **two** distinct groups of MPI processes linked with a context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547554b-860b-4bcd-941a-89320e3265b0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Collectives Categories\n",
    "\n",
    "MPI intra-communicator (and almost inter...) collective operations fits **4** categories:\n",
    "\n",
    "* **All-To-All**: All MPI processes contribute to the result. All MPI processes receive the result.\n",
    "* **All-To-One**: All MPI processes contribute to the result. One MPI process (**root**) receives the result.\n",
    "* **One-To-All**: One MPI process contributes to the result. All MPI processes receive the result.\n",
    "* **Other**: Collective operations that do not fit into one of the above categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769eaa38-cfcb-47a5-bdbc-b1e7c7cfb800",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Collectives in an image...\n",
    "\n",
    "<img src=\"./Images/collectives.png\" width=30% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35471a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Barriers\n",
    "\n",
    "* _Intra-Comm_: stop a group of processes until they are **synchronized**.\n",
    "* _Inter-Comm_: the call returns for group **A** if all members of group **B** have entered the call (and viceversa) \n",
    "\n",
    "> ...An MPI process may return from the call before all MPI processes in its own group have entered the call...\n",
    "\n",
    "`MPI_Barrier`: see https://www.open-mpi.org/doc/v4.1/man3/MPI_Barrier.3.php\n",
    "\n",
    "<img src=\"./Images/barrier.png\" width=50% style=\"margin-left:auto; margin-right:auto\">\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>Severe</b> performance impact if used too often. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67973ed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Broadcast\n",
    "\n",
    "* _Intra-Comm_: broadcasts a message from the **root** process to **all other processes** of the group.\n",
    "* _Inter-Comm_: group **A** is the **root**. All MPI processes in group **B** pass in argument root the rank of the root in group **A**. The root passes the value `MPI_ROOT` in root. All other MPI processes in group A pass the value `MPI_PROC_NULL` in root. Data is broadcast from the **root** to all MPI processes in group B.\n",
    "\n",
    "`MPI_Bcast`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Bcast.3.php\n",
    "\n",
    "<img src=\"./Images/bcast.png\" width=40% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7dcd26",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile main_bcast.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    \n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    \n",
    "    MPI_Status status;\n",
    "    double a[2] = { 0.0, 0.0 };\n",
    "    if ( rank == 0 ) \n",
    "    {\n",
    "        a[0] = 2.1; \n",
    "        a[1] = 4.3;\n",
    "    }\n",
    "    \n",
    "    // send the information to all the other processes\n",
    "    MPI_Bcast(a, 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
    "    \n",
    "    std::cout<< \"Process \"<< rank<< \" \";\n",
    "    std::cout<< \"a \"<< a[0]<< \", \"<< a[1]<< std::endl; \n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf53e29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_bcast\n",
    "cd debug_bcast\n",
    "cmake -DSOURCES=\"main_bcast.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2f0b1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_bcast\n",
    "mpirun -np 4 3_Collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9e63e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gather and Scatter (1)\n",
    "\n",
    "* _Intra-Comm_: the **root** process collects data elements from all the processes and stores them in rank order (**Gather**) and viceversa (**Scatter**).\n",
    "* _Inter-Comm_: group **A** defines the **root**. All MPI processes in group **B** pass the same value in argument root, which is the rank of the root in group **A**. The root passes the value `MPI_ROOT` in root. All other MPI processes in group A\n",
    "pass the value `MPI_PROC_NULL` in root. Data is gathered from all MPI processes in group **B** to the root (and viceversa)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589d52b-4bb7-4fa5-81bc-209c5a2c39e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gather and Scatter (2)\n",
    "  \n",
    "* `MPI_Gather`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Gather.3.php\n",
    "* `MPI_Scatter`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Scatter.3.php\n",
    "\n",
    "<img src=\"./Images/gather.png\" width=48% style=\"float: left;\">  \n",
    "<img src=\"./Images/scatter.png\" width=48% style=\"float: right;\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85f865",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile main_gather_scatter.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    \n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    \n",
    "    MPI_Status status;\n",
    "    int a[8] = { 0 };\n",
    "    if (rank == 0) \n",
    "    {\n",
    "        for (unsigned int i = 0; i < 8; i++)\n",
    "            a[i] = i + 1; \n",
    "    }\n",
    "    \n",
    "    // send the information to all the other processes\n",
    "    MPI_Scatter(a, 2, MPI_INT, a, 2, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "    \n",
    "    std::cout<< \"Before Process \"<< rank<< \" \";\n",
    "    std::cout<< \"a: \";\n",
    "    for (unsigned int i = 0; i < 8; i++)\n",
    "        std::cout<< (i == 0 ? \"\" : \", \")<< a[i];\n",
    "    std::cout<< std::endl;\n",
    "        \n",
    "    a[0] *= 2;\n",
    "    a[1] *= 2;\n",
    "    \n",
    "    // get the information from the other processes\n",
    "    MPI_Gather(a, 2, MPI_INT, a, 2, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "    \n",
    "    // stop processes to obtain a good output\n",
    "    MPI_Barrier(MPI_COMM_WORLD);\n",
    "    \n",
    "    std::cout<< \"After Process \"<< rank<< \" \";\n",
    "    std::cout<< \"a: \";\n",
    "    for (unsigned int i = 0; i < 8; i++)\n",
    "        std::cout<< (i == 0 ? \"\" : \", \")<< a[i];\n",
    "    std::cout<< std::endl;\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a580c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_gather_scatter\n",
    "cd debug_gather_scatter\n",
    "cmake -DSOURCES=\"main_gather_scatter.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a937128",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_gather_scatter\n",
    "mpirun -np 4 3_Collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca34d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GatherV and ScatterV\n",
    "\n",
    "More **complex** gather and scatter call where itâ€™s possible to define a different length of arrays.\n",
    "\n",
    "* `MPI_GatherV`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Gatherv.3.php\n",
    "* `MPI_ScatterV`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Scatterv.3.php\n",
    "\n",
    "<img src=\"./Images/scatterv.png\" width=50% style=\"margin-left:auto; margin-right:auto\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d670a36a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other collectives\n",
    "\n",
    "* `MPI_Allgather`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Allgather.3.php\n",
    "* `MPI_Alltoall`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Alltoall.3.php\n",
    "* `MPI_Allgatherv`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Allgatherv.3.php\n",
    "* `MPI_Alltoallv`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Alltoallv.3.php\n",
    "* ...\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"><b>NOTE</b>: expensive calls - use only when needed.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30189c45-ce84-4709-bfc0-9f3a11bb2230",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `MPI_Allgather` - Intra-Comm\n",
    "\n",
    "<img src=\"./Images/allgather.png\" width=48% style=\"margin-left:auto; margin-right:auto\">\n",
    "\n",
    "### `MPI_Allgather` - Inter-Comm\n",
    "\n",
    "<img src=\"./Images/all_gather_inter.png\" width=48% style=\"margin-left:auto; margin-right:auto\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b700c90c-9038-4cba-8af8-4e0d292f5932",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### One-Way Inter-Communication\n",
    "\n",
    "> ...When a complete exchange is executed in the inter-communicator case, then the number of data items sent from MPI processes in group **A** to MPI processes in group **B** need **NOT** equal the number of items sent in the reverse direction.\n",
    "In particular, one can have unidirectional communication by specifying `sendcount = 0` in the reverse direction..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbd272",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reductions\n",
    "\n",
    "<img src=\"./Images/reduce.png\" width=23% style=\"float: right;\">  \n",
    "\n",
    "A **reduction** takes values from a group of processes and generates a **single value** with some operation (e.g. a `sum`, `average`, etc.).\n",
    "\n",
    "* collect data from different processes;\n",
    "* store the result on a single process or distribute the value to all processes\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Designed to <b>avoid race-conditions</b>.</div>\n",
    "\n",
    "* `MPI_Reduce`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Reduce.3.php\n",
    "* `MPI_Allreduce`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Allreduce.3.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a91bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile main_reduce.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    \n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    \n",
    "    MPI_Status status;\n",
    "    int a[2] = { rank, rank + 1 };\n",
    "    int sum[2] = { 0, 0 };\n",
    "    \n",
    "    std::cout<< \"Before Process \"<< rank<< \" \";\n",
    "    std::cout<< \"a: \"<< a[0]<< \", \"<< a[1]<< std::endl;\n",
    "    \n",
    "    MPI_Reduce(&a, &sum, 2, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
    "            \n",
    "    std::cout<< \"After Process \"<< rank<< \" \";\n",
    "    std::cout<< \"sum: \"<< sum[0]<< \", \"<< sum[1]<< std::endl;\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1589a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_reduce\n",
    "cd debug_reduce\n",
    "cmake -DSOURCES=\"main_reduce.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9250267",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_reduce\n",
    "mpirun -np 4 3_Collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8347dd5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reduction Operations\n",
    "<img src=\"./Images/reduction.png\" width=50% style=\"margin-left:auto; margin-right:auto\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42afd2-ea10-4a4e-ac86-e10931e429b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reduction Operations (2)\n",
    "\n",
    "* To use `MPI_MINLOC` and `MPI_MAXLOC` in a reduce operation, one must provide a datatype argument that represents a **pair** (value and index), as `MPI_DOUBLE_INT`\n",
    "* User can define **Reduction Operations** with function `MPI_OP_CREATE`\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>REMEMBER</b>: use <code>MPI_OP_ FREE</code> to remove it </div>\n",
    "\n",
    "* `MPI_SCAN` is an interesting function, see the example  \n",
    "\n",
    "```\n",
    " * +---------------+   +---------------+   +---------------+   +---------------+\n",
    " * | MPI process 0 |   | MPI process 1 |   | MPI process 2 |   | MPI process 3 |\n",
    " * +---------------+   +---------------+   +---------------+   +---------------+\n",
    " * |       0       |   |       1       |   |       2       |   |       3       |\n",
    " * +-------+-------+   +-------+-------+   +-------+-------+   +-------+-------+\n",
    " *         |                   |                   |                   |\n",
    " *         |                +--+--+                |                   |\n",
    " *         +----------------| SUM |                |                   |\n",
    " *         |                +--+--+                |                   |\n",
    " *         |                   |                +--+--+                |\n",
    " *         |                   +----------------| SUM |                |\n",
    " *         |                   |                +--+--+                |\n",
    " *         |                   |                   |                +--+--+\n",
    " *         |                   |                   +----------------| SUM |\n",
    " *         |                   |                   |                +--+--+\n",
    " *         |                   |                   |                   |\n",
    " * +-------+-------+   +-------+-------+   +-------+-------+   +-------+-------+\n",
    " * |       0       |   |       1       |   |       3       |   |       6       |\n",
    " * +---------------+   +---------------+   +---------------+   +---------------+\n",
    " * | MPI process 0 |   | MPI process 1 |   | MPI process 2 |   | MPI process 3 |\n",
    " * +---------------+   +---------------+   +---------------+   +---------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8446636-56f9-4f9f-82be-1f3ee0c5aa2b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inter-Comm Reduction Operations\n",
    "\n",
    "* for reduce opeartion:\n",
    "> ...If comm is an inter-communicator, then the call involves all MPI processes in the inter-communicator, but with group **A** defining the root. All MPI processes in the group **B** pass the same value in argument root, which is the rank of the **root** in group A. The root passes the value `MPI_ROOT` in root. All other MPI processes in group **A** pass the value `MPI_PROC_NULL` in root. Only send buffer arguments are significant in group **B** and only receive buffer arguments are significant at the root....\n",
    "* for All reduce operations\n",
    "> ...If comm is an inter-communicator, then the result of the reduction of the data provided by MPI processes in group **A** is stored at each MPI process in group **B**, and vice versa. Both groups should provide count and datatype arguments that specify the same type signa..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe9bad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Collectives and Performance\n",
    "\n",
    "MPI vendors work **hard** to optimise collectives for parallel hardware.\n",
    "\n",
    "_Latency measurements_ (minimum time needed to transfer data) in different machines: image taken from: https://doi.org/10.1002/cpe.6769\n",
    "<img src=\"./Images/performance.png\" width=80% style=\"margin-left:auto; margin-right:auto\"> \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> <b>NOTE</b>: parallel scaling is often dictated by MPI collectives. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b514548-e676-44a2-80bf-da75b7ddd4bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Nonblocking and Persistent Collectives\n",
    "\n",
    "> ...The nonblocking collective communication model is similar to the model used for non-blocking point-to-point communication...\n",
    "\n",
    "> ...It is erroneous to call `MPI_REQUEST_FREE` or `MPI_CANCEL` for a request associated with a nonblocking collective operation. Nonblocking collective requests created using the APIs described in this section are not persistent. However, persistent collective requests can be created using persistent collective operations...\n",
    "\n",
    "> ..nonblocking collective operations do not match with blocking collective operations, and collective operations do not have a **tag** argument. All\n",
    "MPI processes must call collective operations (blocking and nonblocking) in the same order per communicator. In particular, once a MPI process calls a collective operation, all other MPI processes in the communicator must eventually call the same collective operation, and no other collective operation with the same communicator in between. This is consistent with the ordering rules for blocking collective operations in threaded environments..."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
