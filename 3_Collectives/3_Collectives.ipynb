{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c49911",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "############# Markdown note ##################\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Use blue boxes for Tips and notes. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Use green boxes sparingly, and only for some specific purpose that the other boxes can't cover. For example, if you have a lot of related content to link to, maybe you decide to use green boxes for related links from each section of a notebook. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> Use yellow boxes for examples that are not inside code cells, or use for mathematical formulas if needed. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> In general, just avoid the red boxes. </div>\n",
    "\n",
    "<img src=\"<path>\" width=20% style=\"margin-left:auto; margin-right:auto\">\n",
    "<img src=\"<path>\" width=40% style=\"float: right;\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91509d85",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# reset all programs\n",
    "rm -rf debug*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371497d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MPI Collectives\n",
    "\n",
    "Collective Communications with **Message Passing Interface** (MPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ded22",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Communications involving groups of processes are called **collectives**.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><code>MPI 1.0-2.0</code> collective calls are blocking. <code>MPI-3</code> introduced <b>non-blocking</b> collectives.</div>\n",
    "\n",
    "They have the following characteristics:\n",
    "* **Every** process in the communicator shall call the collective function;\n",
    "* **No tags** are required.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Designed to replace loops of point-to-point calls to be <b>more efficient</b>. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19624b43",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Barriers\n",
    "\n",
    "To stops a group of processes until they are **synchronized**.\n",
    "\n",
    "* `MPI_Barrier`: see https://www.open-mpi.org/doc/v4.1/man3/MPI_Barrier.3.php\n",
    "\n",
    "<img src=\"./Images/barrier.png\" width=50% style=\"margin-left:auto; margin-right:auto\">\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>Severe</b> performance impact if used too often. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bdd66d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Broadcast\n",
    "\n",
    "Broadcasts a message from a process to **all other processes** of the group.\n",
    "\n",
    "* `MPI_Bcast`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Bcast.3.php\n",
    "\n",
    "<img src=\"./Images/bcast.png\" width=40% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b7dcd26",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main_bcast.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile main_bcast.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    \n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    \n",
    "    MPI_Status status;\n",
    "    double a[2] = { 0.0, 0.0 };\n",
    "    if ( rank == 0 ) \n",
    "    {\n",
    "        a[0] = 2.1; \n",
    "        a[1] = 4.3;\n",
    "    }\n",
    "    \n",
    "    // send the information to all the other processes\n",
    "    MPI_Bcast(a, 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
    "    \n",
    "    std::cout<< \"Process \"<< rank<< \" \";\n",
    "    std::cout<< \"a \"<< a[0]<< \", \"<< a[1]<< std::endl; \n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf53e29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/geoscore/Desktop/GEO++/Courses/PhdMPI/3_Collectives/debug_bcast\n",
      "Consolidate compiler generated dependencies of target 3_Collectives\n",
      "[ 50%] Building CXX object CMakeFiles/3_Collectives.dir/main_bcast.cpp.o\n",
      "[100%] Linking CXX executable 3_Collectives\n",
      "[100%] Built target 3_Collectives\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_bcast\n",
    "cd debug_bcast\n",
    "cmake -DSOURCES=\"main_bcast.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddc2f0b1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0 a 2.1, 4.3\n",
      "Process 1 a 2.1, 4.3\n",
      "Process 2 a 2.1, 4.3\n",
      "Process 3 a 2.1, 4.3\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_bcast\n",
    "mpirun -np 4 3_Collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff96d04",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gather and Scatter\n",
    "\n",
    "One process collects data elements from all the processes and stores them in rank order (**Gather**) and viceversa (**Scatter**).\n",
    "\n",
    "* `MPI_Gather`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Gather.3.php\n",
    "* `MPI_Scatter`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Scatter.3.php\n",
    "\n",
    "<img src=\"./Images/gather.png\" width=48% style=\"float: left;\">  \n",
    "<img src=\"./Images/scatter.png\" width=48% style=\"float: right;\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cdc7927",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main_gather_scatter.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile main_gather_scatter.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    \n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    \n",
    "    MPI_Status status;\n",
    "    int a[8] = { 0 };\n",
    "    if ( rank == 0 ) \n",
    "    {\n",
    "        for (unsigned int i = 0; i < 8; i++)\n",
    "            a[i] = i + 1; \n",
    "    }\n",
    "    \n",
    "    // send the information to all the other processes\n",
    "    MPI_Scatter(a, 2, MPI_INT, a, 2, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "    \n",
    "    std::cout<< \"Before Process \"<< rank<< \" \";\n",
    "    std::cout<< \"a: \";\n",
    "    for (unsigned int i = 0; i < 8; i++)\n",
    "        std::cout<< (i == 0 ? \"\" : \", \")<< a[i];\n",
    "    std::cout<< std::endl;\n",
    "        \n",
    "    a[0] *= 2;\n",
    "    a[1] *= 2;\n",
    "    \n",
    "    // get the information from the other processes\n",
    "    MPI_Gather(a, 2, MPI_INT, a, 2, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "    \n",
    "    // stop processes to obtain a good output\n",
    "    MPI_Barrier(MPI_COMM_WORLD);\n",
    "    \n",
    "    std::cout<< \"After Process \"<< rank<< \" \";\n",
    "    std::cout<< \"a: \";\n",
    "    for (unsigned int i = 0; i < 8; i++)\n",
    "        std::cout<< (i == 0 ? \"\" : \", \")<< a[i];\n",
    "    std::cout<< std::endl;\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "510277f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/geoscore/Desktop/GEO++/Courses/PhdMPI/3_Collectives/debug_gather_scatter\n",
      "Consolidate compiler generated dependencies of target 3_Collectives\n",
      "[ 50%] Building CXX object CMakeFiles/3_Collectives.dir/main_gather_scatter.cpp.o\n",
      "[100%] Linking CXX executable 3_Collectives\n",
      "[100%] Built target 3_Collectives\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_gather_scatter\n",
    "cd debug_gather_scatter\n",
    "cmake -DSOURCES=\"main_gather_scatter.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "579c919c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Process 0 a: 1, 2, 3, 4, 5, 6, 7, 8\n",
      "Before Process 1 a: 3, 4, 0, 0, 0, 0, 0, 0\n",
      "Before Process 2 a: 5, 6, 0, 0, 0, 0, 0, 0\n",
      "Before Process 3 a: 7, 8, 0, 0, 0, 0, 0, 0\n",
      "After Process 0 a: 2, 4, 6, 8, 10, 12, 14, 16\n",
      "After Process 1 a: 6, 8, 0, 0, 0, 0, 0, 0\n",
      "After Process 2 a: 10, 12, 0, 0, 0, 0, 0, 0\n",
      "After Process 3 a: 14, 16, 0, 0, 0, 0, 0, 0\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_gather_scatter\n",
    "mpirun -np 4 3_Collectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e149a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GatherV and ScatterV\n",
    "\n",
    "More **complex** gather and scatter call where it’s possible to define a different length of arrays.\n",
    "\n",
    "* `MPI_GatherV`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Gatherv.3.php\n",
    "* `MPI_ScatterV`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Scatterv.3.php\n",
    "\n",
    "<img src=\"./Images/scatterv.png\" width=50% style=\"margin-left:auto; margin-right:auto\"> "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
