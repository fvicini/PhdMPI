{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de70423",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "############# Markdown note ##################\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Use blue boxes for Tips and notes. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Use green boxes sparingly, and only for some specific purpose that the other boxes can't cover. For example, if you have a lot of related content to link to, maybe you decide to use green boxes for related links from each section of a notebook. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> Use yellow boxes for examples that are not inside code cells, or use for mathematical formulas if needed. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> In general, just avoid the red boxes. </div>\n",
    "\n",
    "<img src=\"<path>\" width=20% style=\"margin-left:auto; margin-right:auto\">\n",
    "<img src=\"<path>\" width=40% style=\"float: right;\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91509d85",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# reset all programs\n",
    "rm -rf debug*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371497d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MPI Advanced\n",
    "\n",
    "Advanced concept of **Message Passing Interface** (MPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526d145",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General DataTypes\n",
    "\n",
    "A **general datatype** is an _opaque_ object that specifies two things:\n",
    "\n",
    "1. A sequence of **basic datatypes**.\n",
    "2. A sequence of integer (byte) **displacements**.\n",
    "\n",
    "> ...We call such a pair of sequences (or sequence of pairs) a **type map**. The sequence of basic datatypes (displacements ignored) is the **type signature**...\n",
    "\n",
    "The **extent** of a datatype is defined to be the span from the first byte to the last byte occupied by entries in this datatype, **rounded up** to satisfy alignment requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995845c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Previously on DataTypes...\n",
    "\n",
    "DataTypes can be created with different MPI routines, for example:\n",
    "\n",
    "* `MPI_Pack`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Pack.3.php\n",
    "* `MPI_Type_create_struct`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Type_create_struct.3.php\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Before using a new DataType, we shall <b>commit</b> it. </div>\n",
    "\n",
    "* `MPI_Type_commit`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Type_commit.3.php\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>REMARK</b>: Once a new data type is created we shall <b>destroy</b> it before closing the application:</div>\n",
    "\n",
    "* `MPI_Type_free`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Type_free.3.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0538d65",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Type_Contiguous\n",
    "\n",
    "Simplest constructor. Makes count **copies** of an existing datatype\n",
    "\n",
    "* `MPI_Type_contiguous`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Type_contiguous.3.php\n",
    "\n",
    "<img src=\"./Images/contigous.png\" width=80% style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cba101",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Type_Vector\n",
    "\n",
    "Like contiguous, but allows for regular gaps (**stride**) in the displacements. \n",
    "\n",
    "* `MPI_Type_vector`, `MPI_Type_hvector`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Type_vector.3.php\n",
    "\n",
    "<img src=\"./Images/vector.png\" width=60% style=\"margin-left:auto; margin-right:auto\">\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b>: For <code>MPI_Type_hvector</code> the stride is specified in bytes.. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad7f14",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Type_Index\n",
    "\n",
    "An array of **non regular displacements** of the input data type is provided as the map for the new data type.\n",
    "\n",
    "* `MPI_Type_indexed`, `MPI_Type_hindexed`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Type_indexed.3.php\n",
    "\n",
    "<img src=\"./Images/indexed.png\" width=60% style=\"margin-left:auto; margin-right:auto\">\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b>: For <code>MPI_Type_hindexed</code> offsets are specified in byte. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7dcd26",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile main_vector.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    \n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    \n",
    "    MPI_Status status;\n",
    "    double a[4][4];\n",
    "    for (unsigned int i = 0; i < 4; i++)\n",
    "        for (unsigned int j = 0; j < 4; j++)\n",
    "            a[i][j] = (rank == 0) ? i * 4.0 + j : 0.0;\n",
    "    \n",
    "    MPI_Datatype rowType, colType;\n",
    "    \n",
    "    MPI_Type_contiguous(4, MPI_DOUBLE, &rowType);\n",
    "    MPI_Type_vector(4, 2, 4, MPI_DOUBLE, &colType);\n",
    "    \n",
    "    MPI_Type_commit(&rowType);\n",
    "    MPI_Type_commit(&colType);\n",
    "    \n",
    "    if (rank == 0)\n",
    "    {\n",
    "        MPI_Send(&a[2][0], 1, rowType, 1, 10, MPI_COMM_WORLD);\n",
    "        MPI_Send(&a[0][2], 1, colType, 1, 11, MPI_COMM_WORLD);\n",
    "    }\n",
    "    else if (rank == 1)\n",
    "    {\n",
    "        MPI_Recv(&a[2][0], 1, rowType, 0, 10, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        MPI_Recv(&a[0][2], 1, colType, 0, 11, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    }\n",
    "    \n",
    "    std::cout<< \"Process \"<< rank<< \": \"<< std::endl;\n",
    "    for (unsigned int i = 0; i < 4; i++)\n",
    "    {\n",
    "        for (unsigned int j = 0; j < 4; j++)\n",
    "            std::cout<< (j == 0 ? \"\" : \", \")<< a[i][j];\n",
    "        std::cout<< std::endl;\n",
    "    }\n",
    "    \n",
    "    MPI_Type_free(&rowType);\n",
    "    MPI_Type_free(&colType);\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf53e29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_vector\n",
    "cd debug_vector\n",
    "cmake -DSOURCES=\"main_vector.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2f0b1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_vector\n",
    "mpirun -np 2 4_MPI_Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96017a96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d03f3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Virtual topology vs Physical Topology\n",
    "\n",
    "MPI **Topology**, or **Virtual Topology** is a process arrangement in topological patterns. \n",
    "\n",
    "Examples are 2D or 3D **grid**, or more generally can be described by a **graph**.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"><b>NOTE</b>: Virtual topology can be exploited by the system in the assignment of processes to physical processors (<b>Physical Topology</b>). This helps to improve the\n",
    "communication performance on a given machine.</div>\n",
    "\n",
    "* `MPI_Cart_create`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Cart_create.3.php\n",
    "* `MPI_Graph_create`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Graph_create.3.php\n",
    "* ...\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">Topology information is always associated with a <b>new communicator</b>. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33383ee6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### REMARK\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">Once a new communicator is created we shall <b>destroy</b> it before closing the application:</div>\n",
    "\n",
    "* `MPI_Comm_free`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Comm_free.3.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf0e0f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example of MPI Graph Topology\n",
    "\n",
    "`MPI_Cart_create`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Cart_create.3.php\n",
    "\n",
    "<img src=\"./Images/graph.png\" width=40% style=\"margin-left:auto; margin-right:auto\">\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Periodicity can be selected for each direction. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cebf3fc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cart Topology Utilities\n",
    "\n",
    "* `MPI_Dims_Create`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Dims_create.3.php - compute optimal balanced distribution of processes per coordinate direction with respect constraints;\n",
    "* `MPI_Cart_coords`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Cart_coords.3.php - given a rank, returns process's coordinates\n",
    "* `MPI_Cart_rank`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Cart_rank.3.php - given process's coordinates, returns the rank\n",
    "* `MPI_Cart_shift`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Cart_shift.3.php - get source and destination rank ids in SendRecv operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e9194",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile main_cart.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    \n",
    "    int rank, nprocs;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n",
    "    \n",
    "    int dim[] = { 0, 0 };\n",
    "    MPI_Dims_create(nprocs, 2, dim); // create 2D cart dimensions\n",
    "    \n",
    "    int period[] = {1, 0}; // periodic in the first dimension\n",
    "    MPI_Comm grid_comm;\n",
    "    MPI_Cart_create(MPI_COMM_WORLD, 2, dim, period, 1, &grid_comm);\n",
    "    \n",
    "    int new_rank;\n",
    "    int coordinates[2] = { 0, 0 };\n",
    "    MPI_Comm_rank(grid_comm, &new_rank); // new rank in the communicator\n",
    "    MPI_Cart_coords(grid_comm, new_rank, 2, coordinates); // coordinate in the grid\n",
    "    \n",
    "    std::cout<< \"Process \"<< rank<< \": \"<< \" \";\n",
    "    std::cout<< \"new_rank \"<< new_rank<< \" \";\n",
    "    std::cout<< \"coordinates \"<< coordinates[0]<< \", \"<< coordinates[1]<< std::endl;\n",
    "    \n",
    "    MPI_Barrier(grid_comm);\n",
    "    \n",
    "    int source, dest;\n",
    "    for (int direction = 0; direction < 2; direction++) \n",
    "    {\n",
    "        for (int displacement = -1; displacement < 2; displacement += 2) \n",
    "        {\n",
    "            int bufferSend = new_rank, bufferRecv = -1;\n",
    "            \n",
    "            MPI_Cart_shift(grid_comm, direction, displacement, &source, &dest);\n",
    "                        \n",
    "            MPI_Sendrecv(&bufferSend, 1, MPI_INT, source, 10, \n",
    "                         &bufferRecv, 1, MPI_INT, dest, 10,\n",
    "                         grid_comm, MPI_STATUS_IGNORE);\n",
    "            \n",
    "            std::cout<< \"Process \"<< rank<< \" - \";\n",
    "            std::cout<< \"direction \"<< direction<< \" \";\n",
    "            std::cout<< \"displacement \"<< displacement<< \": \";\n",
    "            std::cout<< \"send to \"<< dest<< \" \";\n",
    "            std::cout<< \"data \"<< bufferSend<< \"; \";\n",
    "            std::cout<< \"recv from \"<< source<< \" \";\n",
    "            std::cout<< \"data \"<< bufferRecv<< std::endl;\n",
    "        }\n",
    "    }\n",
    "           \n",
    "    MPI_Comm_free(&grid_comm);\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55889f81",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_cart\n",
    "cd debug_cart\n",
    "cmake -DSOURCES=\"main_cart.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6958126f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_cart\n",
    "mpirun --oversubscribe -np 4 4_MPI_Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717dfe5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>NOTE</b>: negative rank can be used in send/recv. Those values correspond to constant <code>MPI_PROC_NULL</code>, ignored by communications operations </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50f76f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Circular Graph\n",
    "\n",
    "<img src=\"./Images/ring.png\" width=30% style=\"float: right;\">\n",
    "\n",
    "**Circular shift** is another typical MPI communication pattern.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">Such a pattern is nothing more than a <b>1D cartesian grid</b> topology with optional periodicity.</div>\n",
    "\n",
    "<img src=\"./Images/cart1d.png\" width=50% style=\"margin-left:auto; margin-right:auto\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0acc8c9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neighbour Communications\n",
    "\n",
    "`MPI-3.0` introduced **advanced communications** for topologies:\n",
    "\n",
    "* `MPI_NEIGHBOR_ALL(GATHER[V] | TOALL[V])`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Neighbor_allgather.3.php - neighbor collective communications, optimized because the communication pattern is known statically\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>NOTE:</b>There are also the <b>non-blocking</b> collective communications <code>MPI_INEIGHBOR_ALL(GATHER[V] | TOALL[V])</code>. </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3b90c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One side communications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9421cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Standard point-to-point communications are **two-sided**: there can be a delay if the sender has to wait to send the data because the receiver is not ready.\n",
    "\n",
    "`MPI-2` standard added Remote Memory Access (**RMA**), also called **one-sided communication**.\n",
    "\n",
    "`MPI-3` further extended RMA to improve functionality and performances.\n",
    "\n",
    "<img src=\"./Images/oneside.png\" width=50% style=\"margin-left:auto; margin-right:auto\">\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>GOAL</b>: to <b>decouple</b> data transfer from system synchronisation. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1224ca2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pros and cons of RMA\n",
    "\n",
    "**Advantages**:\n",
    "* <div class=\"alert alert-block alert-success\">Only <b>one process</b> taking part performance should be greater: no implicit synchronization, all data movement routines are non-blocking;</div>\n",
    "* <div class=\"alert alert-block alert-success\">Programs are more <b>easily written</b> with RMA: similar to <b>shared-memory</b> and opposed to message passing.</div>\n",
    "\n",
    "**Disadvantages**:\n",
    "* <div class=\"alert alert-block alert-danger\">The programmer shall specify the <b>synchronization</b> of the processes to write in the public memory;</div>\n",
    "* <div class=\"alert alert-block alert-danger\">In practice RMA may not be <b>faster</b>, if compared to well written P2P.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9d60b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Main RMA Routines\n",
    "\n",
    "Routines to manage the RMA shared memory window:\n",
    "* `MPI_Win_create`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Win_create.3.php\n",
    "* `MPI_Win_fence`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Win_fence.3.php\n",
    "* `MPI_Win_free`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Win_free.3.php\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"><b>REMARK</b>: each window created shall be <b>destroyed</b>.</div>\n",
    "\n",
    "Rountes to manage the RMA communications:\n",
    "* `MPI_Put`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Put.3.php\n",
    "* `MPI_Get`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Get.3.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180da0e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile main_rma.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    \n",
    "    int rank, nprocs;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    \n",
    "    MPI_Win window;\n",
    "    int shared_buffer[2] = { rank, rank };\n",
    "    // create a simple RMA window of 2 integers\n",
    "    MPI_Win_create(shared_buffer, sizeof(int) * 2, sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &window);\n",
    "    \n",
    "    MPI_Win_fence(0, window); // synchronization\n",
    "    \n",
    "    int local_buffer[2] = { rank + 2, rank + 4 };\n",
    "    int get_local_buffer[2] = { 0, 0 };\n",
    "    \n",
    "    if (rank == 0)\n",
    "        MPI_Get(get_local_buffer, 2, MPI_INT, 1, 0, 2, MPI_INT, window);\n",
    "    else if (rank == 1)\n",
    "        MPI_Get(get_local_buffer, 2, MPI_INT, 0, 0, 2, MPI_INT, window);\n",
    "    \n",
    "    MPI_Win_fence(0, window); // synchronization\n",
    "    \n",
    "    std::cout<< \"Process \"<< rank<< \": \";\n",
    "    std::cout<< \"shared_buffer \"<< shared_buffer[0]<< \" \"<< shared_buffer[1]<< \" \";\n",
    "    std::cout<< \"local_buffer \"<< local_buffer[0]<< \" \"<< local_buffer[1]<< \" \";\n",
    "    std::cout<< \"get_local_buffer \"<< get_local_buffer[0]<< \" \"<< get_local_buffer[1]<< std::endl;\n",
    "    \n",
    "    if (rank == 0)\n",
    "        MPI_Put(local_buffer, 2, MPI_INT, 1, 0, 2, MPI_INT, window);\n",
    "    else if (rank == 1)\n",
    "        MPI_Put(local_buffer, 2, MPI_INT, 0, 0, 2, MPI_INT, window);\n",
    "    \n",
    "    MPI_Win_fence(0, window); // synchronization\n",
    "    \n",
    "    if (rank == 0)\n",
    "        MPI_Get(get_local_buffer, 2, MPI_INT, 0, 0, 2, MPI_INT, window);\n",
    "    else if (rank == 1)\n",
    "        MPI_Get(get_local_buffer, 2, MPI_INT, 1, 0, 2, MPI_INT, window);\n",
    "    \n",
    "    MPI_Win_fence(0, window); // synchronization\n",
    "    \n",
    "    std::cout<< \"Process \"<< rank<< \": \";\n",
    "    std::cout<< \"shared_buffer \"<< shared_buffer[0]<< \" \"<< shared_buffer[1]<< \" \";\n",
    "    std::cout<< \"local_buffer \"<< local_buffer[0]<< \" \"<< local_buffer[1]<< \" \";\n",
    "    std::cout<< \"get_local_buffer \"<< get_local_buffer[0]<< \" \"<< get_local_buffer[1]<< std::endl;\n",
    "    \n",
    "    MPI_Win_free(&window);\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae986f11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_rma\n",
    "cd debug_rma\n",
    "cmake -DSOURCES=\"main_rma.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1652cf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_rma\n",
    "mpirun -np 2 4_MPI_Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d740245",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dynamic processes in MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded1622",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Normally MPI tasks are **fixed** (e.g. by mpirun) at the start of execution.\n",
    "\n",
    "`MPI-2` provides calls to create processes \"**on the fly**\".\n",
    "\n",
    "* `MPI_COMM_SPAWN`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Comm_spawn.3.php - new set of processes with the same command lines (SPMD)\n",
    "* `MPI_COMM_SPAWN_MULTIPLE`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Comm_spawn_multiple.3.php - new set of processes with potentially different command (MPMD)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>NOTE</b>: not all MPI implementations support MPI spawning. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"><b>Not commonly used</b> in HPC environments.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3e160",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile main_spawn.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    MPI_Init(&argc, &argv);\n",
    "    \n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    \n",
    "    int numSpawn = 2;\n",
    "    MPI_Comm parentcomm, intercomm;\n",
    "    int errcodes[numSpawn];\n",
    "    \n",
    "    MPI_Comm_get_parent(&parentcomm);\n",
    "    \n",
    "    if (parentcomm == MPI_COMM_NULL)\n",
    "    {\n",
    "        // Create 2 more processes\n",
    "        MPI_Comm_spawn( \"./4_MPI_Advanced\", MPI_ARGV_NULL, numSpawn, MPI_INFO_NULL, \n",
    "                       0, MPI_COMM_WORLD, &intercomm, errcodes);\n",
    "        \n",
    "        std::cout<< \"Parent Process \"<< rank<< std::endl;\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        int subRank;\n",
    "        MPI_Comm_rank(parentcomm, &subRank);\n",
    "        std::cout<< \"Spawned Process \"<< rank<< \" subRank \"<< subRank<< std::endl;\n",
    "    }\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5d4c3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_spawn\n",
    "cd debug_spawn\n",
    "cmake -DSOURCES=\"main_spawn.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a771d3f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_spawn\n",
    "mpirun -np 1 4_MPI_Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30ed726",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hybrid OpenMP & MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3072cc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Two-level Parallelization:\n",
    "* MPI **between nodes**;\n",
    "* OpenMP **within shared-memory nodes**.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">Each process can have <b>multiple</b> threads executing simultaneously</div>\n",
    "\n",
    "* `MPI_Init_thread`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Init_thread.3.php\n",
    "\n",
    "4 levels of thread safety\n",
    "* `MPI_THREAD_SINGLE` - One thread exists in program (classic MPI)\n",
    "* `MPI_THREAD_FUNNELED` - Multithreaded but only the master thread can make MPI calls\n",
    "* `MPI_THREAD_SERIALIZED` - Multithreaded, but only one thread can make MPI calls at a time\n",
    "* `MPI_THREAD_MULTIPLE` - Multithreaded and any thread can make MPI calls at any time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5239bd4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile main_thread.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{    \n",
    "    int provided;\n",
    "    MPI_Init_thread(&argc, &argv, MPI_THREAD_FUNNELED, &provided);\n",
    "    \n",
    "    if (provided != MPI_THREAD_FUNNELED)\n",
    "        std::cerr<< \"Warning MPI did not provide MPI_THREAD_FUNNELED\"<< std::endl;\n",
    "    \n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    \n",
    "    if (rank == 0)\n",
    "        omp_set_num_threads(3); // set number of threads on each process\n",
    "    else if (rank == 1)\n",
    "        omp_set_num_threads(2); // set number of threads on each process\n",
    "    \n",
    "    std::cout<< \"Process \"<< rank<< \" numThreads \"<<omp_get_num_threads()<< std::endl;\n",
    "    #pragma omp parallel default(none), shared(rank), shared(ompi_mpi_comm_world, std::cout)\n",
    "    {\n",
    "        int flag;\n",
    "        MPI_Is_thread_main(&flag);\n",
    "    \n",
    "        #pragma omp critical\n",
    "        std::cout<< \"Process \"<< rank<< \" \"\n",
    "        << \"numThreads \"<<omp_get_num_threads()<< \" \"\n",
    "        << \"thread \"<< omp_get_thread_num()<< \" \"\n",
    "        << \"isMainThread \"<< flag<< std::endl;\n",
    "        \n",
    "        #pragma omp master\n",
    "        {\n",
    "            // do some MPI operation with master thread\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b4082",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_thread\n",
    "cd debug_thread\n",
    "cmake -DSOURCES=\"main_thread.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618a8bc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_thread\n",
    "mpirun -np 2 4_MPI_Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407db65-4e8f-4316-ac79-4ed5760eb667",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Partitioned Point-to-Point Communication\n",
    "\n",
    "Partitioned communication **extends** persistent point-to-point communication (use a persistent communication style).\n",
    "\n",
    "> ...Advice to users. The techniques of partitioned communication were known as `fine-points` before their adoption into the MPI standard...\n",
    "\n",
    "Partitioned communication is different in three fundamental ways from persistent point-to-point operations in MPI:\n",
    "\n",
    "1. allows **additional partitioned test function** calls that can expose partial completion of the operation\n",
    "2. **may** perform all of the initialization required to enable data transfer as early as its initialization phase\n",
    "3. allows for MPI to be **independently notified** of multiple contributions from the send-side to a single data buffer of a single MPI message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd32cea-bee5-4482-b57b-dc7e063b1b5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile main_partitioned.cpp\n",
    "\n",
    "#include <stdlib.h>\n",
    "#include <mpi.h>\n",
    "#include <omp.h>\n",
    "#define PARTITIONS 8\n",
    "#define COUNT 5\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{    \n",
    "    int provided;\n",
    "    MPI_Init_thread(&argc, &argv, MPI_THREAD_SERIALIZED, &provided);\n",
    "    \n",
    "    double message[PARTITIONS * COUNT];\n",
    "    MPI_Count partitions = PARTITIONS;\n",
    "    int source = 0 , dest = 1, tag = 1 , flag = 0;\n",
    "    MPI_Request request;\n",
    "\n",
    "    if (provided < MPI_THREAD_SERIALIZED)\n",
    "        MPI_Abort(MPI_COMM_WORLD , EXIT_FAILURE);\n",
    "\n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "    if (rank == 0)\n",
    "    {\n",
    "        MPI_Psend_init(message, partitions, COUNT, MPI_DOUBLE, dest, tag, MPI_COMM_WORLD, MPI_INFO_NULL, &request);\n",
    "        MPI_Start(&request);\n",
    "        for (int i = 0; i < partitions ; ++i)\n",
    "        {\n",
    "            // compute and fill partition #i, then mark ready :\n",
    "            MPI_Pready(i , request);\n",
    "        }\n",
    "\n",
    "        while (!flag)\n",
    "        {\n",
    "            // do useful work #1\n",
    "            MPI_Test(&request, &flag, MPI_STATUS_IGNORE);\n",
    "            // do useful work #2\n",
    "        }\n",
    "\n",
    "        MPI_Request_free(&request);\n",
    "    }\n",
    "    else if (myrank == 1)\n",
    "    {\n",
    "        MPI_Precv_init(message, partitions, COUNT, MPI_DOUBLE, source, tag, MPI_COMM_WORLD, MPI_INFO_NULL, &request);\n",
    "        MPI_Start(&request);\n",
    "        while (!flag)\n",
    "        {\n",
    "            // do useful work #1\n",
    "            MPI_Test(&request, &flag, MPI_STATUS_IGNORE);\n",
    "            // do useful work #2\n",
    "        }\n",
    "        MPI_Request_free(&request);\n",
    "    }\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33058492",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Debugging and Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c8f0c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Debugging\n",
    "\n",
    "Debugging in serial can be tricky: \n",
    "* errors;\n",
    "* uninitialized variables;\n",
    "* stack smashing;\n",
    "* ...\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">Debugging in parallel adds <b>multiple different dimensions</b> to this problem: </div>\n",
    "\n",
    "* race conditions;\n",
    "* asynchronous events;\n",
    "* deadlocks;\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396d069",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Profiling\n",
    "\n",
    "MPI functions to measure **elapsed time** in parellel:\n",
    "\n",
    "* `MPI_Wtime`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Wtime.3.php\n",
    "* `MPI_Wtick`: https://www.open-mpi.org/doc/v4.1/man3/MPI_Wtick.3.php - resolution of `MPI_Wtime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b60dbd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile main_time.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <unistd.h> // library for usleep\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{    \n",
    "    MPI_Init(&argc, &argv);\n",
    "        \n",
    "    int rank;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    \n",
    "    MPI_Barrier(MPI_COMM_WORLD);\n",
    "    double startTime = MPI_Wtime(); // start time\n",
    "    \n",
    "    int seconds = 1 + rank; \n",
    "    usleep(seconds * 1000 * 1000); // takes microseconds\n",
    "    \n",
    "    double endTime = MPI_Wtime(); // end time\n",
    "    double elapsedTime = endTime - startTime;\n",
    "    \n",
    "    double maxElapsedTime = 0.0;\n",
    "    MPI_Allreduce(&elapsedTime, &maxElapsedTime, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n",
    "    \n",
    "    std::cout.precision(4);\n",
    "    std::cout<< std::scientific<< \"Process \"<< rank<< \" \";\n",
    "    std::cout<< \"LocalTime \"<< elapsedTime<< \" secs\"<< \" \";\n",
    "    std::cout<< \"MaxTime \"<< maxElapsedTime<< \" secs\"<< std::endl;\n",
    "    \n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db9fe3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# compile program\n",
    "mkdir -p ./debug_time\n",
    "cd debug_time\n",
    "cmake -DSOURCES=\"main_time.cpp\" ..\n",
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476847e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# run program\n",
    "cd debug_time\n",
    "mpirun -np 4 4_MPI_Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd4a4d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Old debugging/profiling methods still works:\n",
    "\n",
    "* **Trial and error**: on code lines `cout` and `cerr`;\n",
    "* `valgrind`: https://valgrind.org/\n",
    "\n",
    "### Available Software (NOT FREE)\n",
    "\n",
    "Available **debuggers**:\n",
    "\n",
    "* `Arm DDT`: https://www.arm.com/products/development-tools/server-and-hpc/forge/ddt\n",
    "* `Totalview`: https://totalview.io/video-tutorials/introduction-mpi-debugging\n",
    "\n",
    "Available **profiles**:\n",
    "\n",
    "* `Intel VTune`: https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
